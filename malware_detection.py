# -*- coding: utf-8 -*-

# from google.colab import drive
# drive.mount('/content/drive')

# !unzip -qq "drive/My Drive/dataset/malevis_train_val_224x224.zip"

# !pip install scikit-plot

# !rm -r /content/malevis_train_val_224x224/train/Other
# !rm -r /content/malevis_train_val_224x224/val/Other

"""### Import libraries"""

import os
import pandas as pd
import seaborn as sns
import numpy as np
from PIL import Image
import tensorflow as tf
import scikitplot as skplt
import seaborn as sns
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense, Dropout, LSTM, Flatten
from tensorflow.keras import Sequential, Model
from tensorflow.keras.layers import Input, Lambda, BatchNormalization, Reshape
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.applications import ResNet152V2, DenseNet169, VGG16
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import GlobalAveragePooling2D
from sklearn.metrics import accuracy_score, f1_score, auc
import matplotlib.pyplot as plt
plt.style.use('ggplot')

"""### Prepare the data"""

train_dir = "/content/malevis_train_val_224x224/train"
val_dir = "/content/malevis_train_val_224x224/val"

train_data = tf.keras.preprocessing.image_dataset_from_directory(
    train_dir,
    labels="inferred",
    label_mode="int",
    class_names=None,
    color_mode="rgb",
    batch_size=32,
    image_size=(224, 224),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation="bilinear",
    follow_links=False,
)

train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=30,
        horizontal_flip=True,
        vertical_flip=True)
test_datagen = ImageDataGenerator(rescale=1./255)

train_gen = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    color_mode="rgb",
    classes=None,
    class_mode="categorical",
    batch_size=32,
    shuffle=True,
    seed=None,
    interpolation="nearest",
)

test_gen = test_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    color_mode="rgb",
    classes=None,
    class_mode="categorical",
    batch_size=32,
    shuffle=False,
    seed=None,
    interpolation="nearest",
)

label_to_name = {k:v for v, k in train_gen.class_indices.items()}

"""### Data visualization
Samples from images that used in the training
"""

class_names = train_data.class_names

plt.figure(figsize=(15, 10))
images = train_gen[0][0]
labels = train_gen[0][1]
for i in range(20):
  ax = plt.subplot(4, 5, i + 1)
  label = np.argmax(labels[i], axis=0)
  plt.imshow(images[i])
  plt.title(class_names[label])
  plt.axis("off")

"""### Download the pre-trained weights"""

denesnet_imagenet_model = DenseNet169(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
resnet_model = ResNet152V2(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

"""### Custom CNN model"""

def custom_cnn(input_shape, n_classes):
    inputs = Input(input_shape)
    x = Conv2D(32, kernel_size = (3, 3), activation='relu')(inputs)
    x = MaxPooling2D(pool_size=(2,2))(x)
    x = BatchNormalization()(x)
    x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    x = BatchNormalization()(x)
    x = Conv2D(96, kernel_size=(3,3), activation='relu')(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    x = BatchNormalization()(x)
    x = Conv2D(32, kernel_size=(3,3), activation='relu')(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    x = BatchNormalization()(x)
    x = Flatten()(x)
    x = Dense(100, activation='relu')(x)
    x = Dropout(0.2)(x)
    x = Dense(50, activation='relu')(x)
    x = Dropout(0.2)(x)
    output = Dense(n_classes, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=output)
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

custom_cnn_model = custom_cnn(input_shape=(224, 224, 3), n_classes=25)

custom_cnn_model.summary()

custom_cnn_hist = custom_cnn_model.fit(train_gen,
                              steps_per_epoch=32,
                              epochs=80,
                              validation_data=test_gen,
                              validation_batch_size=10)

fig, ax = plt.subplots(1,2, figsize=[12,6])
ax[0].plot(custom_cnn_hist.history["loss"])
ax[0].plot(custom_cnn_hist.history["val_loss"])
ax[0].legend(('Training', 'Validation'),loc='upper right')
ax[0].set_title(" Loss")
ax[0].set_xlabel("Epochs")
ax[1].plot(custom_cnn_hist.history['accuracy'])
ax[1].plot(custom_cnn_hist.history["val_accuracy"])
ax[1].legend(('Training', 'Validation'),loc='lower right')
ax[1].set_title("Training Accuracy")
ax[1].set_xlabel("Epochs")

preds = custom_cnn_model.predict_generator(test_gen)

y_preds = np.argmax(preds, axis=1).flatten()
y=test_gen.classes

ax=skplt.metrics.plot_confusion_matrix(y, y_preds, figsize=(20, 10), normalize=True,
                                       title='Custom CNN model confusion matrix')
tickx=ax.set_xticklabels(train_data.class_names)
ticky=ax.set_yticklabels(train_data.class_names)
# Rotate the tick labels and set their alignment.
plot=plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")

print(classification_report(y, y_preds, target_names=train_data.class_names))

"""### Classifier module"""

def classifier(n_classes, base_model, optimizer=Adam(lr=0.001),
               num_units=80, dropout=0.2, trainable=False):

    for layer in base_model.layers:
        layer.trainable = trainable

    features = base_model.output
    features = GlobalAveragePooling2D()(features)
    output = Dense(num_units, activation='relu')(features)
    output = Dropout(dropout)(output)
    output = Dense(40, activation='relu')(output)
    output = Dropout(dropout)(output)
    output = Dense(n_classes, activation='softmax')(output)
    model = Model(inputs=base_model.input, outputs=output)

    model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

"""### Denesnet based model"""

denesnet_based_model = classifier(
                                  n_classes=25,
                                  base_model=denesnet_imagenet_model,
                                  )

denesnet_hist = denesnet_based_model.fit(train_gen,
                                         steps_per_epoch=32,
                                         epochs=30,
                                         validation_data=test_gen,
                                         validation_batch_size=10)

fig, ax = plt.subplots(1,2, figsize=[12,6])
ax[0].plot(denesnet_hist.history["loss"])
ax[0].plot(denesnet_hist.history["val_loss"])
ax[0].legend(('Training', 'Validation'),loc='upper right')
ax[0].set_title(" Loss")
ax[0].set_xlabel("Epochs")
ax[1].plot(denesnet_hist.history['accuracy'])
ax[1].plot(denesnet_hist.history["val_accuracy"])
ax[1].legend(('Training', 'Validation'),loc='lower right')
ax[1].set_title("Training Accuracy")
ax[1].set_xlabel("Epochs")

preds = denesnet_based_model.predict_generator(test_gen)

y_preds = np.argmax(preds, axis=1).flatten()
y=test_gen.classes

ax=skplt.metrics.plot_confusion_matrix(y, y_preds, figsize=(20, 10), normalize=True,
                                       title='Denesnet based model confusion matrix')
tickx=ax.set_xticklabels(train_data.class_names)
ticky=ax.set_yticklabels(train_data.class_names)
# Rotate the tick labels and set their alignment.
plot=plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")

print(classification_report(y, y_preds, target_names=train_data.class_names))

"""### Resnet based model"""

resnet_based_model = classifier(n_classes=25,
                                  base_model=resnet_model,
                                  )

resnet_hist = resnet_based_model.fit(train_gen,
                                         steps_per_epoch=32,
                                         epochs=30,
                                         validation_data=test_gen,
                                         validation_batch_size=10)

fig, ax = plt.subplots(1,2, figsize=[12,6])
ax[0].plot(resnet_hist.history["loss"])
ax[0].plot(resnet_hist.history["val_loss"])
ax[0].legend(('Training', 'Validation'),loc='upper right')
ax[0].set_title(" Loss")
ax[0].set_xlabel("Epochs")
ax[1].plot(resnet_hist.history['accuracy'])
ax[1].plot(resnet_hist.history["val_accuracy"])
ax[1].legend(('Training', 'Validation'),loc='lower right')
ax[1].set_title("Training Accuracy")
ax[1].set_xlabel("Epochs")

preds = resnet_based_model.predict_generator(test_gen)

y_preds = np.argmax(preds, axis=1).flatten()
y=test_gen.classes

ax=skplt.metrics.plot_confusion_matrix(y, y_preds, figsize=(20, 10), normalize=True,
                                       title='Resnet based model confusion matrix')
tickx=ax.set_xticklabels(train_data.class_names)
ticky=ax.set_yticklabels(train_data.class_names)
# Rotate the tick labels and set their alignment.
plot=plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")

print(classification_report(y, y_preds, target_names=train_data.class_names))

"""
### VGG based model

"""

vgg_based_model = classifier( n_classes=25,
                              base_model=vgg_model,
                            )

vgg_hist = vgg_based_model.fit(train_gen,
                              steps_per_epoch=32,
                              epochs=30,
                              validation_data=test_gen,
                              validation_batch_size=10)

fig, ax = plt.subplots(1,2, figsize=[12,6])
ax[0].plot(vgg_hist.history["loss"])
ax[0].plot(vgg_hist.history["val_loss"])
ax[0].legend(('Training', 'Validation'),loc='upper right')
ax[0].set_title(" Loss")
ax[0].set_xlabel("Epochs")
ax[1].plot(vgg_hist.history['accuracy'])
ax[1].plot(vgg_hist.history["val_accuracy"])
ax[1].legend(('Training', 'Validation'),loc='lower right')
ax[1].set_title("Training Accuracy")
ax[1].set_xlabel("Epochs")

preds = vgg_based_model.predict_generator(test_gen)

y_preds = np.argmax(preds, axis=1).flatten()
y=test_gen.classes

ax=skplt.metrics.plot_confusion_matrix(y, y_preds, figsize=(20, 10), normalize=True,
                                       title='VGG based model confusion matrix')
tickx=ax.set_xticklabels(train_data.class_names)
ticky=ax.set_yticklabels(train_data.class_names)
# Rotate the tick labels and set their alignment.
plot=plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")

print(classification_report(y, y_preds, target_names=train_data.class_names))

"""### Denesnet with LSTM"""

def denesnet_lstm(n_classes, base_model, optimizer=Adam(lr=0.001),
                  dropout=0.2, trainable=False):

    for layer in base_model.layers:
        layer.trainable = trainable

    features = base_model.output
    features = GlobalAveragePooling2D()(features)
    features = Reshape((1, -1))(features)
    output = LSTM(128, input_shape=(224*224, 1), return_sequences=True)(features)
    output = LSTM(64)(output)
    output = Dense(40, activation='relu')(output)
    output = Dropout(dropout)(output)
    output = Dense(n_classes, activation='softmax')(output)
    model = Model(inputs=base_model.input, outputs=output)

    model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    return model

denesnet_lstm_model = denesnet_lstm(n_classes=25,
                                    base_model=denesnet_imagenet_model)

denesnet_lstm_hist = denesnet_lstm_model.fit(train_gen,
                              steps_per_epoch=32,
                              epochs=30,
                              validation_data=test_gen,
                              validation_batch_size=10)

fig, ax = plt.subplots(1,2, figsize=[12,6])
ax[0].plot(denesnet_lstm_hist.history["loss"])
ax[0].plot(denesnet_lstm_hist.history["val_loss"])
ax[0].legend(('Training', 'Validation'),loc='upper right')
ax[0].set_title(" Loss")
ax[0].set_xlabel("Epochs")
ax[1].plot(denesnet_lstm_hist.history['accuracy'])
ax[1].plot(denesnet_lstm_hist.history["val_accuracy"])
ax[1].legend(('Training', 'Validation'),loc='lower right')
ax[1].set_title("Training Accuracy")
ax[1].set_xlabel("Epochs")

preds = denesnet_lstm_model.predict_generator(test_gen)

y_preds = np.argmax(preds, axis=1).flatten()
y=test_gen.classes

ax=skplt.metrics.plot_confusion_matrix(y, y_preds, figsize=(20, 10), normalize=False,
                                       title='DenesNet with LSTM model confusion matrix')
tickx=ax.set_xticklabels(train_data.class_names)
ticky=ax.set_yticklabels(train_data.class_names)
# Rotate the tick labels and set their alignment.
plot=plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")

print(classification_report(y, y_preds, target_names=train_data.class_names))

"""### Custum CNN model with LSTM"""

def custum_cnn_lstm(input_shape, n_classes):
    inputs = Input(input_shape)
    x = Conv2D(32, kernel_size = (3, 3), activation='relu')(inputs)
    x = MaxPooling2D(pool_size=(2,2))(x)
    x = BatchNormalization()(x)
    x = Conv2D(64, kernel_size=(3,3), activation='relu')(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    x = BatchNormalization()(x)
    x = Conv2D(96, kernel_size=(3,3), activation='relu')(x)
    x = MaxPooling2D(pool_size=(2,2))(x)
    x = BatchNormalization()(x)
    x = Conv2D(32, kernel_size=(3,3), activation='relu')(x)
    x = Flatten()(x)
    x = Reshape((1, -1))(x)
    x = LSTM(64)(x)
    x = Dense(100, activation='relu')(x)
    x = Dropout(0.2)(x)
    x = Dense(50, activation='relu')(x)
    x = Dropout(0.2)(x)
    output = Dense(n_classes, activation='softmax')(x)
    model = Model(inputs=inputs, outputs=output)
    model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

custum_cnn_lstm_model = custum_cnn_lstm(input_shape=(224, 224, 3), n_classes=25)

custum_cnn_lstm_model.summary()

custum_cnn_lstm_hist = custum_cnn_lstm_model.fit(train_gen,
                              steps_per_epoch=32,
                              epochs=120,
                              validation_data=test_gen,
                              validation_batch_size=10)

fig, ax = plt.subplots(1,2, figsize=[12,6])
ax[0].plot(custum_cnn_lstm_hist.history["loss"])
ax[0].plot(custum_cnn_lstm_hist.history["val_loss"])
ax[0].legend(('Training', 'Validation'),loc='upper right')
ax[0].set_title(" Loss")
ax[0].set_xlabel("Epochs")
ax[1].plot(custum_cnn_lstm_hist.history['accuracy'])
ax[1].plot(custum_cnn_lstm_hist.history["val_accuracy"])
ax[1].legend(('Training', 'Validation'),loc='lower right')
ax[1].set_title("Training Accuracy")
ax[1].set_xlabel("Epochs")

preds = custum_cnn_lstm_model.predict_generator(test_gen)

y_preds = np.argmax(preds, axis=1).flatten()
y=test_gen.classes

ax=skplt.metrics.plot_confusion_matrix(y, y_preds, figsize=(20, 10), title='Resnet based model confusion matrix')
tickx=ax.set_xticklabels(train_data.class_names)
ticky=ax.set_yticklabels(train_data.class_names)
# Rotate the tick labels and set their alignment.
plot=plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         rotation_mode="anchor")

"""### Compareing models performance"""

models_results = pd.read_csv('model_results.csv')

models_results.head()

plt.figure(figsize=(15, 10))


for col in models_results.columns[1:]:
  plt.plot(models_results['Unnamed: 0'],
         models_results[col],
         label = col)
  plt.scatter(models_results['Unnamed: 0'],
         models_results[col])
plt.xticks(models_results['Unnamed: 0'], models_results['Unnamed: 0'], rotation='vertical')
plt.xlabel('Malware family')
plt.ylabel('Accuracy')

plt.legend()